# -*- coding: utf-8 -*-
"""Flirty vs Non-Flirty text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LR60Xv5A1-ujOaLpfpL4VcUGmp2xArYh
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import BaggingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC  # Support Vector Classifier for SVM
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from nltk.stem import PorterStemmer
import spacy
from sklearn.ensemble import RandomForestClassifier
from scipy.stats import randint as sp_randint
from sklearn.model_selection import RandomizedSearchCV

data = pd.read_csv(r'/content/FlirtyOrNot.csv', encoding='ISO-8859-1')

data

"""Pre-processing

Make text to lower case
"""

data['Text'] = data['Text'].str.replace("[^a-zA-Z]", " ",regex=True).str.lower()

"""Remove stopwords"""

data['Text'] = data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (ENGLISH_STOP_WORDS)]))

"""Stemming"""

stemmer = PorterStemmer()
data['Text'] = data['Text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))

"""Lemmatization"""

!python -m spacy download en_core_web_sm
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])

data['Text'] = data['Text'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))

"""TF-IDF with n-gram"""

tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
X = tfidf_vectorizer.fit_transform(data['Text']).toarray()
y = data['Label'].values

data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""PCA to understand if we have linear or non-linear data"""

pca = PCA(n_components=2)
X_train_reduced = pca.fit_transform(X_train)

plt.figure(figsize=(10, 6))
for label, color in zip(range(2), ('blue', 'red')):
    plt.scatter(X_train_reduced[y_train == label, 0],
                X_train_reduced[y_train == label, 1],
                label=f'Class {label}',
                color=color)

plt.title('TF-IDF Features Reduced to 2D using PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

"""Naive Bayes"""

MNB_model = MultinomialNB()

MNB_model.fit(X_train, y_train)

"""Grid Search for Single Naive Bayes"""

alphas = np.logspace(-2, 1, 20)
param_grid = {'alpha': alphas}
grid_search = GridSearchCV(estimator=MNB_model, param_grid=param_grid, n_jobs=-1, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best Alpha: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)

best_mnb = MultinomialNB(alpha=grid_search.best_params_['alpha'])
best_mnb.fit(X_train, y_train)

"""Evaluation for training data- Single Naive Bayes"""

y_train_NB_pred = best_mnb.predict(X_train)
accuracy_NB_train = accuracy_score(y_train, y_train_NB_pred)
print(f"Accuracy: {accuracy_NB_train*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_train, y_train_NB_pred))

"""Evaluation for test set- Single Naive Bayes"""

y_test_NB_pred = best_mnb.predict(X_test)
accuracy_NB_test = accuracy_score(y_test, y_test_NB_pred)
print(f"Accuracy: {accuracy_NB_test*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_test_NB_pred))

"""Cross Validation for Single Naive Bayes"""

n_folds = 5

scores_NB = cross_val_score(best_mnb, X, y, cv=n_folds)
print(f"Accuracy scores for each fold: {scores_NB}")
print(f"Mean cross-validation accuracy: {scores_NB.mean():.2f}")
print(f"Standard deviation of cross-validation accuracy: {scores_NB.std():.2f}")

"""Bagging classifier as ensemble model for Naive Bayes"""

base_model = best_mnb
bagging_clf = BaggingClassifier(estimator=base_model, n_estimators=10, random_state=42)
bagging_clf.fit(X_train, y_train)

"""Evaluation for Bagging for Naive Bayes- Train"""

y_train_NB_bagging = bagging_clf.predict(X_train)
accuracy_NB_bagging_train = accuracy_score(y_train, y_train_NB_bagging)
print(f"Accuracy: {accuracy_NB_bagging_train*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_train, y_train_NB_bagging))

"""Evaluation for Bagging for Naive Bayes- Test"""

y_test_NB_bagging = bagging_clf.predict(X_test)
accuracy_NB_bagging_test = accuracy_score(y_test, y_test_NB_bagging)
print(f"Accuracy: {accuracy_NB_bagging_test*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_test_NB_bagging))

"""Logistic Regression"""

LR_model = LogisticRegression(random_state=42, max_iter=1000)
LR_model.fit(X_train, y_train)

"""Grid search for Logistic Regression"""

param_grid = {
    'C': np.logspace(-4, 4, 20),
    'penalty': ['l1', 'l2', 'elasticnet'],
    'solver': ['saga'],  # 'saga' solver supports all penalty types
    'class_weight': [None, 'balanced'],
    'l1_ratio': np.linspace(0, 1, 10)  # Only used with 'elasticnet' penalty
}
grid_search = GridSearchCV(estimator=LR_model, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy', verbose=2)
grid_search.fit(X_train, y_train)
print("Best Parameters: ", grid_search.best_params_)
print("Best Cross-Validation Score: ", grid_search.best_score_)

"""Re-training based on grid search result"""

LR_model = LogisticRegression(
    C=11.288378916846883,
    class_weight=None,  # Explicitly specifying None for class_weight
    l1_ratio=0.1111111111111111,
    penalty='elasticnet',
    solver='saga',
    max_iter=1000,  # Ensuring a high number of iterations for convergence
    random_state=42  # For reproducibility
)
LR_model.fit(X_train, y_train)

"""Evaluation for training data- Logistic Regression"""

y_train_LR_pred = LR_model.predict(X_train)
accuracy_LR_train = accuracy_score(y_train, y_train_LR_pred)
print(f"Accuracy: {accuracy_LR_train*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_train, y_train_LR_pred))

"""Evaluation for test set- Logistic Regression"""

y_test_LR_pred = LR_model.predict(X_test)
accuracy_LR_test = accuracy_score(y_test, y_test_LR_pred)
print(f"Accuracy: {accuracy_LR_test*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_test_LR_pred))

"""Cross Validation for Logistic Regression"""

scores_LR = cross_val_score(LR_model, X, y, cv=n_folds)
print(f"Accuracy scores for each fold: {scores_LR}")
print(f"Mean cross-validation accuracy: {scores_LR.mean():.2f}")
print(f"Standard deviation of cross-validation accuracy: {scores_LR.std():.2f}")

"""Support Vector Machine"""

SVM_model = SVC(C=0.6,random_state=42,kernel='rbf')
SVM_model.fit(X_train, y_train)

"""Grid search SVM"""

parameters = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto'],
}

grid_search_SVM = GridSearchCV(SVM_model, parameters, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)

grid_search_SVM.fit(X_train, y_train)

print("Best parameters:", grid_search_SVM.best_params_)
print("Best score:", grid_search_SVM.best_score_)

"""Update the model according to Grid Search"""

SVM_model = SVC(C=10,random_state=42,kernel='linear',gamma='scale')
SVM_model.fit(X_train, y_train)

"""Evaluation for training data- SVM"""

y_train_SVM_pred = SVM_model.predict(X_train)
accuracy_SVM_train = accuracy_score(y_train, y_train_SVM_pred)
print(f"Accuracy: {accuracy_SVM_train*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_train, y_train_SVM_pred))

"""Evaluation for test set- SVM"""

y_test_SVM_pred = SVM_model.predict(X_test)
accuracy_SVM_test = accuracy_score(y_test, y_test_SVM_pred)
print(f"Accuracy: {accuracy_SVM_test*100:.2f}%")
print("\nClassification Report:\n", classification_report(y_test, y_test_SVM_pred))

"""Cross Validation for SVM"""

scores_SVM = cross_val_score(SVM_model, X, y, cv=n_folds)
print(f"Accuracy scores for each fold: {scores_SVM}")
print(f"Mean cross-validation accuracy: {scores_SVM.mean():.2f}")
print(f"Standard deviation of cross-validation accuracy: {scores_SVM.std():.2f}")

unseen_data = pd.read_csv(r'/content/FlirtyOrNot_Test.csv', encoding='ISO-8859-1')

unseen_data['Text'] = unseen_data['Text'].str.replace("[^a-zA-Z]", " ",regex=True).str.lower()

unseen_data['Text'] = unseen_data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (ENGLISH_STOP_WORDS)]))
stemmer = PorterStemmer()
unseen_data['Text'] = unseen_data['Text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
unseen_data['Text'] = unseen_data['Text'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))

unseen_data_tfidf = tfidf_vectorizer.transform(unseen_data['Text']).toarray()

unseen_data_tfidf

"""Unseen data prediction with Single NB"""

new_predictions_NB = best_mnb.predict(unseen_data_tfidf)

accuracy_new_predictions_NB = accuracy_score(unseen_data['Label'], new_predictions_NB)
print(f"Accuracy for Single NB: {accuracy_new_predictions_NB*100:.2f}%")
print("\nClassification Report for Single NB:\n", classification_report(unseen_data['Label'], new_predictions_NB))

"""Unseen data prediction with Bagging NB"""

new_predictions_Bagging_NB = bagging_clf.predict(unseen_data_tfidf)
accuracy_new_predictions_Bagging_NB = accuracy_score(unseen_data['Label'], new_predictions_Bagging_NB)
print(f"Accuracy for Bagging NB: {accuracy_new_predictions_Bagging_NB*100:.2f}%")
print("\nClassification Report for Bagging NB:\n", classification_report(unseen_data['Label'], new_predictions_Bagging_NB))

new_predictions_LR = LR_model.predict(unseen_data_tfidf)

accuracy_new_predictions_LR = accuracy_score(unseen_data['Label'], new_predictions_LR)
print(f"Accuracy: {accuracy_new_predictions_LR*100:.2f}%")
print("\nClassification Report:\n", classification_report(unseen_data['Label'], new_predictions_LR))

new_predictions_SVM = SVM_model.predict(unseen_data_tfidf)

accuracy_new_predictions_SVM = accuracy_score(unseen_data['Label'], new_predictions_SVM)
print(f"Accuracy: {accuracy_new_predictions_SVM*100:.2f}%")
print("\nClassification Report:\n", classification_report(unseen_data['Label'], new_predictions_SVM))

"""Random Forest classifier"""

rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=42)

rf_classifier.fit(X_train, y_train)

rf_predictions = rf_classifier.predict(X_test)

rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f'Random Forest Accuracy: {rf_accuracy}')

new_predictions_RF = rf_classifier.predict(unseen_data_tfidf)

accuracy_new_predictions_RF = accuracy_score(unseen_data['Label'], new_predictions_RF)
print(f"Accuracy: {accuracy_new_predictions_RF*100:.2f}%")
print("\nClassification Report:\n", classification_report(unseen_data['Label'], new_predictions_RF))

#attempted SVM, Random Forest, Logistic Regression, Naive Bayes and other classification algorithms, but no proper result. Skipping to LSTM now

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping

# Load and preprocess data
data = pd.read_csv(r'/content/FlirtyOrNot.csv', encoding='ISO-8859-1')
data['Text'] = data['Text'].str.replace("[^a-zA-Z]", " ", regex=True).str.lower()
data['Text'] = data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (ENGLISH_STOP_WORDS)]))
stemmer = PorterStemmer()
data['Text'] = data['Text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) 
data['Text'] = data['Text'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))

# Tokenization and padding
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(data['Text'])
sequences = tokenizer.texts_to_sequences(data['Text'])
max_length = max(len(x) for x in sequences)
X = pad_sequences(sequences, maxlen=max_length)
y = data['Label'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build LSTM model
model = Sequential()
max_index_value = max([max(sequence) for sequence in sequences if len(sequence) > 0]) + 1
model.add(Embedding(input_dim=max_index_value, output_dim=50, input_length=max_length))
model.add(LSTM(units=120, return_sequences=False))
model.add(Dropout(0.6))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss',  # Monitor the validation set loss
                               patience=3,          # Number of epochs with no improvement after which training will be stopped
                               verbose=1,           # Verbosity mode
                               mode='min',          # Mode = 'min' because we want to minimize loss; use 'max' for accuracy
                               restore_best_weights=True)  # Restore model weights from the epoch with the best value of the monitored quantity

# Train the model
model.fit(X_train, y_train, epochs=20, batch_size=8, validation_split=0.2, callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {accuracy}')

# Predict on unseen data
unseen_data = pd.read_csv(r'/content/FlirtyOrNot_Test.csv', encoding='ISO-8859-1')
unseen_data['Text'] = unseen_data['Text'].str.replace("[^a-zA-Z]", " ", regex=True).str.lower()
unseen_data['Text'] = unseen_data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (ENGLISH_STOP_WORDS)]))
unseen_data['Text'] = unseen_data['Text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))
unseen_data['Text'] = unseen_data['Text'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))
sequences_unseen = tokenizer.texts_to_sequences(unseen_data['Text'])
X_unseen = pad_sequences(sequences_unseen, maxlen=max_length)
predictions_unseen = model.predict(X_unseen)
predicted_classes_unseen = (predictions_unseen > 0.5).astype(int).flatten()

# Evaluate predictions on unseen data
accuracy_unseen = accuracy_score(unseen_data['Label'], predicted_classes_unseen)
print(f"Accuracy on unseen data: {accuracy_unseen*100:.2f}%")
print("\nClassification Report on unseen data:\n", classification_report(unseen_data['Label'], predicted_classes_unseen))

"""Insertion into MongoDB"""

from pymongo import MongoClient

# Connect to the MongoDB, change the connection string per your MongoDB environment
client = MongoClient('mongodb://localhost:27017/')

# Select the database - if it doesn't exist, it will be created automatically
db = client['prediction_results']

# Select the collection
collection = db['text_predictions']

# Assuming 'unseen_data' is your DataFrame containing the texts and their true labels,
# 'new_predictions_NB', 'new_predictions_Bagging_NB', etc., are your model predictions,
# and 'predicted_classes_unseen' contains your LSTM model predictions,
# you can iterate over the DataFrame and insert the results along with LSTM predictions

for index, row in unseen_data.iterrows():
    document = {
        "text": row['Text'],
        "true_label": int(row['Label']),
        "predictions": {
            "naive_bayes": int(new_predictions_NB[index]),
            "bagging_naive_bayes": int(new_predictions_Bagging_NB[index]),
            "logistic_regression": int(new_predictions_LR[index]),
            "svm": int(new_predictions_SVM[index]),
            "random_forest": int(new_predictions_RF[index]),
            "lstm": int(predicted_classes_unseen[index]),  # Including LSTM predictions
        }
    }
    # Insert the document into the collection
    collection.insert_one(document)

print("Inserted prediction results, including LSTM, into MongoDB.")